{"cells":[{"cell_type":"code","source":["import sempy.fabric as fabric\n","from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n","import pandas as pd\n","import uuid\n","\n","#Instantiate the client\n","client = fabric.FabricRestClient()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e38d8ecb-4fa3-45b1-8932-a94b67bc1389"},{"cell_type":"markdown","source":["# Load dataframe of notebooks to monitor"],"metadata":{},"id":"83644d67-f372-4e00-8edd-fdd297360d5c"},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM OnyxToolsLake.disablednotebookmonitoring\")\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"45c9e1ef-d238-4f08-bb05-b9f87791b15e"},{"cell_type":"code","source":["# Workspace column to list\n","workspace_names = df.select(\"Workspace\").rdd.map(lambda row: row[0]).collect()\n","\n","# Notebook column to list\n","monitoring_list = df.select(\"Notebook\").rdd.map(lambda row: row[0]).collect()\n","\n","print(monitoring_list)\n","print(workspace_names)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"17a0ef16-49c8-49e3-bdd8-a81a576f44c5"},{"cell_type":"markdown","source":["#### Retrieve Workspace IDs"],"metadata":{},"id":"a24edfc2-0435-4040-bf51-2efae4bb133b"},{"cell_type":"code","source":["# Resolve workspace names to IDs, skipping any not found\n","workspace_ids = {}\n","for name in workspace_names:\n","    try:\n","        wid = fabric.resolve_workspace_id(name)\n","        workspace_ids[name] = wid\n","    except WorkspaceNotFoundException:\n","        print(f\"Workspace '{name}' not found. Skipping...\")\n","\n","# Create the workspaces list with both ID and name\n","workspaces = [{\"workspace_id\": wid, \"workspace_name\": name} for name, wid in workspace_ids.items()]\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd028b1c-d465-420c-a5c0-7b2170657e12"},{"cell_type":"code","source":["# Initialize Fabric client\n","client = fabric.FabricRestClient()\n","\n","# Store disabled schedules\n","disabled_schedules = []\n","\n","for workspace in workspaces:\n","    workspace_id = workspace[\"workspace_id\"]\n","    workspace_name = workspace[\"workspace_name\"]\n","\n","    try:\n","        items_url = f\"/v1/workspaces/{workspace_id}/items\"\n","        items_response = client.get(items_url)\n","\n","        if items_response.status_code != 200:\n","            continue\n","\n","        items_data = items_response.json()\n","        items = items_data.get('value', [])\n","\n","        workspace_notebooks = [item for item in items if item.get('type') == 'Notebook']\n","\n","        for notebook in workspace_notebooks:\n","            notebook_id = notebook.get('id')\n","            notebook_name = notebook.get('displayName', 'Unnamed Notebook')\n","\n","            if notebook_name not in monitoring_list:\n","                continue\n","\n","            job_type = \"RunNotebook\"\n","            schedule_url = f\"/v1/workspaces/{workspace_id}/items/{notebook_id}/jobs/{job_type}/schedules\"\n","\n","            try:\n","                schedule_response = client.get(schedule_url)\n","                if schedule_response.status_code == 200:\n","                    schedules = schedule_response.json().get('value', [])\n","\n","                    for s in schedules:\n","                        if not s.get(\"enabled\", False):\n","                            disabled_schedules.append({\n","                                \"WorkspaceID\": workspace_id,\n","                                \"WorkspaceName\": workspace_name,\n","                                \"NotebookID\": notebook_id,\n","                                \"NotebookName\": notebook_name,\n","                                \"ScheduleID\": s.get(\"id\", \"Unknown\"),\n","                                \"Status\": \"Disabled\",\n","                                \"DetectedAt\": pd.Timestamp.now(),\n","                                \"TriggerID\": str(uuid.uuid4())\n","                            })\n","\n","            except Exception as e:\n","                print(f\"Error checking schedule for {notebook_name}: {e}\")\n","\n","    except Exception as e:\n","        print(f\"Error processing workspace {workspace_name}: {e}\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5f5a8fcb-76a7-4cae-8814-c97fdd0c7347"},{"cell_type":"code","source":["if disabled_schedules:\n","    print(f\"✅{len(disabled_schedules)} disabled schedules found\")\n","else:\n","    print(\"ℹ️ No disabled schedules found.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4dcd3945-70ca-4b69-b402-f2133fbdb170"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","import pandas as pd\n","# Convert to Pandas DataFrame\n","disabled_df = pd.DataFrame(disabled_schedules)\n","#display(disabled_df)\n","\n","# Define schema\n","schema = StructType([\n","    StructField(\"WorkspaceID\", StringType(), True),\n","    StructField(\"WorkspaceName\", StringType(), True),\n","    StructField(\"NotebookID\", StringType(), True),\n","    StructField(\"NotebookName\", StringType(), True),\n","    StructField(\"ScheduleID\", StringType(), True),\n","    StructField(\"Status\", StringType(), True),\n","    StructField(\"DetectedAt\", TimestampType(), True),\n","    StructField(\"TriggerID\", StringType(), True)\n","])\n","\n","# Convert to Spark DataFrame, even if empty\n","disabled_spark_df = spark.createDataFrame(disabled_df if not disabled_df.empty else [], schema)\n","# display(disabled_spark_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"36ef6841-52e0-45d4-9025-98b5994e3175"},{"cell_type":"code","source":["import json\n","\n","output = disabled_spark_df.toJSON().collect()\n","mssparkutils.notebook.exit(json.dumps([json.loads(row) for row in output]))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c4a73e51-72f9-41fb-a7ae-2bf670f3cd06"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"83037cfc-6791-4ac4-ae26-38075549718d","default_lakehouse_name":"OnyxToolsLake","default_lakehouse_workspace_id":"0409393c-944d-4b64-9222-d4017c7466d0","known_lakehouses":[{"id":"83037cfc-6791-4ac4-ae26-38075549718d"}]}}},"nbformat":4,"nbformat_minor":5}