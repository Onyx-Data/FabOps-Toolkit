{
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "%run NB - Load Configuration"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "c116c1e1-941c-4618-91a4-7a5343fd244c"
        },
        {
            "cell_type": "code",
            "source": [
                "import sempy.fabric as fabric\n",
                "from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n",
                "import pandas as pd\n",
                "import uuid\n",
                "\n",
                "#Instantiate the client\n",
                "client = fabric.FabricRestClient()"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "ed1fdb23-479b-4ac1-bacd-db71bc6aed74"
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load dataframe of notebooks to monitor"
            ],
            "metadata": {},
            "id": "20e7a6b3-55c8-435a-af39-ee317a066066"
        },
        {
            "cell_type": "code",
            "source": [
                "df = spark.sql(\"SELECT * FROM OnyxToolsLake.disablednotebookmonitoring\")\n",
                "display(df)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "8e8a9864-94a9-45ef-ae3f-d801b245422f"
        },
        {
            "cell_type": "code",
            "source": [
                "# Workspace column to list\n",
                "workspace_names = df.select(\"Workspace\").rdd.map(lambda row: row[0]).collect()\n",
                "\n",
                "# Notebook column to list\n",
                "monitoring_list = df.select(\"Notebook\").rdd.map(lambda row: row[0]).collect()\n",
                "\n",
                "print(monitoring_list)\n",
                "print(workspace_names)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "7c3b3aaa-70aa-4f2e-b409-f85b989655fb"
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Retrieve Workspace IDs"
            ],
            "metadata": {},
            "id": "853e837b-ef57-48a2-a4c0-784c11c7497d"
        },
        {
            "cell_type": "code",
            "source": [
                "# Resolve workspace names to IDs, skipping any not found\n",
                "workspace_ids = {}\n",
                "for name in workspace_names:\n",
                "    try:\n",
                "        wid = fabric.resolve_workspace_id(name)\n",
                "        workspace_ids[name] = wid\n",
                "    except WorkspaceNotFoundException:\n",
                "        print(f\"Workspace '{name}' not found. Skipping...\")\n",
                "\n",
                "# Create the workspaces list with both ID and name\n",
                "workspaces = [{\"workspace_id\": wid, \"workspace_name\": name} for name, wid in workspace_ids.items()]\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "82135fda-2296-4e4b-8849-109496fcc75f"
        },
        {
            "cell_type": "code",
            "source": [
                "# Initialize Fabric client\n",
                "client = fabric.FabricRestClient()\n",
                "\n",
                "# Store disabled schedules\n",
                "disabled_schedules = []\n",
                "\n",
                "for workspace in workspaces:\n",
                "    workspace_id = workspace[\"workspace_id\"]\n",
                "    workspace_name = workspace[\"workspace_name\"]\n",
                "\n",
                "    try:\n",
                "        items_url = f\"/v1/workspaces/{workspace_id}/items\"\n",
                "        items_response = client.get(items_url)\n",
                "\n",
                "        if items_response.status_code != 200:\n",
                "            continue\n",
                "\n",
                "        items_data = items_response.json()\n",
                "        items = items_data.get('value', [])\n",
                "\n",
                "        workspace_notebooks = [item for item in items if item.get('type') == 'Notebook']\n",
                "\n",
                "        for notebook in workspace_notebooks:\n",
                "            notebook_id = notebook.get('id')\n",
                "            notebook_name = notebook.get('displayName', 'Unnamed Notebook')\n",
                "\n",
                "            if notebook_name not in monitoring_list:\n",
                "                continue\n",
                "\n",
                "            job_type = \"RunNotebook\"\n",
                "            schedule_url = f\"/v1/workspaces/{workspace_id}/items/{notebook_id}/jobs/{job_type}/schedules\"\n",
                "\n",
                "            try:\n",
                "                schedule_response = client.get(schedule_url)\n",
                "                if schedule_response.status_code == 200:\n",
                "                    schedules = schedule_response.json().get('value', [])\n",
                "\n",
                "                    for s in schedules:\n",
                "                        if not s.get(\"enabled\", False):\n",
                "                            disabled_schedules.append({\n",
                "                                \"WorkspaceID\": workspace_id,\n",
                "                                \"WorkspaceName\": workspace_name,\n",
                "                                \"NotebookID\": notebook_id,\n",
                "                                \"NotebookName\": notebook_name,\n",
                "                                \"ScheduleID\": s.get(\"id\", \"Unknown\"),\n",
                "                                \"Status\": \"Disabled\",\n",
                "                                \"DetectedAt\": pd.Timestamp.now(),\n",
                "                                \"TriggerID\": str(uuid.uuid4())\n",
                "                            })\n",
                "\n",
                "            except Exception as e:\n",
                "                print(f\"Error checking schedule for {notebook_name}: {e}\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing workspace {workspace_name}: {e}\")\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "b9f398f2-4fac-4579-9048-ab851958d1bb"
        },
        {
            "cell_type": "code",
            "source": [
                "if disabled_schedules:\n",
                "    print(f\"✅{len(disabled_schedules)} disabled schedules found\")\n",
                "else:\n",
                "    print(\"ℹ️ No disabled schedules found.\")"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "7ad8a056-735c-4397-93b1-21f612f30cdb"
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
                "import pandas as pd\n",
                "# Convert to Pandas DataFrame\n",
                "disabled_df = pd.DataFrame(disabled_schedules)\n",
                "#display(disabled_df)\n",
                "\n",
                "# Define schema\n",
                "schema = StructType([\n",
                "    StructField(\"WorkspaceID\", StringType(), True),\n",
                "    StructField(\"WorkspaceName\", StringType(), True),\n",
                "    StructField(\"NotebookID\", StringType(), True),\n",
                "    StructField(\"NotebookName\", StringType(), True),\n",
                "    StructField(\"ScheduleID\", StringType(), True),\n",
                "    StructField(\"Status\", StringType(), True),\n",
                "    StructField(\"DetectedAt\", TimestampType(), True),\n",
                "    StructField(\"TriggerID\", StringType(), True)\n",
                "])\n",
                "\n",
                "# Convert to Spark DataFrame, even if empty\n",
                "disabled_spark_df = spark.createDataFrame(disabled_df if not disabled_df.empty else [], schema)\n",
                "# display(disabled_spark_df)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "198e37b9-6d9c-4a3e-bfd0-5c087177f3d5"
        },
        {
            "cell_type": "code",
            "source": [
                "import json\n",
                "\n",
                "output = disabled_spark_df.toJSON().collect()\n",
                "mssparkutils.notebook.exit(json.dumps([json.loads(row) for row in output]))"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "b40ab640-0b0d-457b-b514-a3395b94fe3a"
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "a365ComputeOptions": null,
        "sessionKeepAliveTimeout": 0,
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "dependencies": {
            "lakehouse": {
                "default_lakehouse": "83037cfc-6791-4ac4-ae26-38075549718d",
                "default_lakehouse_name": "OnyxToolsLake",
                "default_lakehouse_workspace_id": "0409393c-944d-4b64-9222-d4017c7466d0",
                "known_lakehouses": [
                    {
                        "id": "83037cfc-6791-4ac4-ae26-38075549718d"
                    }
                ]
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}