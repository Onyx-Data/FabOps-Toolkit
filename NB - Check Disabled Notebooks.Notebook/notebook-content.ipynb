{
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "%run NB - Load Configuration"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "e365767c-84c0-46fd-a64d-9718890839ed"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Enter the workspace name the notebooks to be monitored are located and also the name of the notebooks to be monitored**"
            ],
            "metadata": {},
            "id": "659dbf92-d75c-42cc-a904-97fb8085c413"
        },
        {
            "cell_type": "code",
            "source": [
                "# Workspace list\n",
                "workspace_names = ['OnyxTools-Test','OnyxTools-Dev']"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "ac78f941-0867-4831-801b-9d2d5601a3bd"
        },
        {
            "cell_type": "code",
            "source": [
                "# Notebook list\n",
                "monitoring_list = ['NB - Dummy Data', 'NB - Sempy Retrieves IDs']"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "604f3383-060b-4ecc-b9bd-7eeb6db7e1ca"
        },
        {
            "cell_type": "code",
            "source": [
                "import sempy.fabric as fabric\n",
                "from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n",
                "import pandas as pd\n",
                "import uuid\n",
                "\n",
                "#Instantiate the client\n",
                "client = fabric.FabricRestClient()"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "212b43af-5c7f-42a3-b425-6f072e23b677"
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Retrieve Workspace IDs"
            ],
            "metadata": {},
            "id": "31b6ce3b-9c97-439c-b7d7-0ea2da83d852"
        },
        {
            "cell_type": "code",
            "source": [
                "# Resolve workspace names to IDs, skipping any not found\n",
                "workspace_ids = {}\n",
                "for name in workspace_names:\n",
                "    try:\n",
                "        wid = fabric.resolve_workspace_id(name)\n",
                "        workspace_ids[name] = wid\n",
                "    except WorkspaceNotFoundException:\n",
                "        print(f\"Workspace '{name}' not found. Skipping...\")\n",
                "\n",
                "# Create the workspaces list with both ID and name\n",
                "workspaces = [{\"workspace_id\": wid, \"workspace_name\": name} for name, wid in workspace_ids.items()]\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "9c77a706-c25f-4a15-9225-f4694e3260ed"
        },
        {
            "cell_type": "code",
            "source": [
                "# Store disabled schedules\n",
                "disabled_schedules = []\n",
                "\n",
                "for workspace in workspaces:\n",
                "    workspace_id = workspace[\"workspace_id\"]\n",
                "    workspace_name = workspace[\"workspace_name\"]\n",
                "\n",
                "    try:\n",
                "        items_url = f\"/v1/workspaces/{workspace_id}/items\"\n",
                "        items_response = client.get(items_url)\n",
                "\n",
                "        if items_response.status_code != 200:\n",
                "            continue\n",
                "\n",
                "        items_data = items_response.json()\n",
                "        items = items_data.get('value', [])\n",
                "\n",
                "        workspace_notebooks = [item for item in items if item.get('type') == 'Notebook']\n",
                "\n",
                "        for notebook in workspace_notebooks:\n",
                "            notebook_id = notebook.get('id')\n",
                "            notebook_name = notebook.get('displayName', 'Unnamed Notebook')\n",
                "\n",
                "            if notebook_name not in monitoring_list:\n",
                "                continue\n",
                "\n",
                "            job_type = \"RunNotebook\"\n",
                "            schedule_url = f\"/v1/workspaces/{workspace_id}/items/{notebook_id}/jobs/{job_type}/schedules\"\n",
                "\n",
                "            try:\n",
                "                schedule_response = client.get(schedule_url)\n",
                "                if schedule_response.status_code == 200:\n",
                "                    schedules = schedule_response.json().get('value', [])\n",
                "\n",
                "                    for s in schedules:\n",
                "                        if not s.get(\"enabled\", False):\n",
                "                            disabled_schedules.append({\n",
                "                                \"WorkspaceID\": workspace_id,\n",
                "                                \"WorkspaceName\": workspace_name,\n",
                "                                \"NotebookID\": notebook_id,\n",
                "                                \"NotebookName\": notebook_name,\n",
                "                                \"ScheduleID\": s.get(\"id\", \"Unknown\"),\n",
                "                                \"Status\": \"Disabled\",\n",
                "                                \"DetectedAt\": pd.Timestamp.now(),\n",
                "                                \"TriggerID\": str(uuid.uuid4())\n",
                "                            })\n",
                "\n",
                "            except Exception as e:\n",
                "                print(f\"Error checking schedule for {notebook_name}: {e}\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing workspace {workspace_name}: {e}\")\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "16fbacbd-c7e4-4d21-8f26-be1cb27292b4"
        },
        {
            "cell_type": "code",
            "source": [
                "if disabled_schedules:\n",
                "    print(f\"✅{len(disabled_schedules)} disabled schedules found\")\n",
                "else:\n",
                "    print(\"ℹ️ No disabled schedules found.\")"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "33a0913c-e1db-42cf-a27c-ac75cf5ad972"
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
                "import pandas as pd\n",
                "# Convert to Pandas DataFrame\n",
                "disabled_df = pd.DataFrame(disabled_schedules)\n",
                "#display(disabled_df)\n",
                "\n",
                "# Define schema\n",
                "schema = StructType([\n",
                "    StructField(\"WorkspaceID\", StringType(), True),\n",
                "    StructField(\"WorkspaceName\", StringType(), True),\n",
                "    StructField(\"NotebookID\", StringType(), True),\n",
                "    StructField(\"NotebookName\", StringType(), True),\n",
                "    StructField(\"ScheduleID\", StringType(), True),\n",
                "    StructField(\"Status\", StringType(), True),\n",
                "    StructField(\"DetectedAt\", TimestampType(), True),\n",
                "    StructField(\"TriggerID\", StringType(), True)\n",
                "])\n",
                "\n",
                "# Convert to Spark DataFrame, even if empty\n",
                "disabled_spark_df = spark.createDataFrame(disabled_df if not disabled_df.empty else [], schema)\n",
                "# display(disabled_spark_df)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "2340eade-e99c-4eb6-9538-c810e30f8049"
        },
        {
            "cell_type": "code",
            "source": [
                "import json\n",
                "\n",
                "output = disabled_spark_df.toJSON().collect()\n",
                "mssparkutils.notebook.exit(json.dumps([json.loads(row) for row in output]))"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "8d40c697-ef46-45ab-a179-76de1ac01438"
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "a365ComputeOptions": null,
        "sessionKeepAliveTimeout": 0,
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "dependencies": {
            "lakehouse": {
                "default_lakehouse": "83037cfc-6791-4ac4-ae26-38075549718d",
                "default_lakehouse_name": "OnyxToolsLake",
                "default_lakehouse_workspace_id": "0409393c-944d-4b64-9222-d4017c7466d0",
                "known_lakehouses": [
                    {
                        "id": "83037cfc-6791-4ac4-ae26-38075549718d"
                    }
                ]
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}