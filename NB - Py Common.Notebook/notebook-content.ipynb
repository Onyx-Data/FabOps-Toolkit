{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernel_info": {
            "name": "synapse_pyspark",
            "jupyter_kernel_name": null
        },
        "a365ComputeOptions": null,
        "sessionKeepAliveTimeout": 0,
        "dependencies": {
            "lakehouse": {
                "default_lakehouse": "83037cfc-6791-4ac4-ae26-38075549718d",
                "default_lakehouse_name": "OnyxToolsLake",
                "default_lakehouse_workspace_id": "0409393c-944d-4b64-9222-d4017c7466d0",
                "known_lakehouses": [
                    {
                        "id": "83037cfc-6791-4ac4-ae26-38075549718d"
                    }
                ]
            }
        }
    },
    "cells": [
        {
            "cell_type": "code",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "source": [
                "%run NB - Load Configuration"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "source": [
                "from pyspark.sql.functions import sha2, concat_ws,current_timestamp,lit,cast,col, when,concat,lpad,year,month, dayofmonth\n",
                "from datetime import datetime"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": null,
            "source": [
                "Convert Date Value to Key for single value"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "source": [
                "def convertValueToKey(value):\n",
                "    key= value.strftime('%Y%m%d')\n",
                "    return int(key)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": null,
            "source": [
                "Add SCD fields to existing dataframe"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "source": [
                "def addSCDFields(df):\n",
                "    df=df.withColumn(\"row_sha2\", sha2(concat_ws(\"||\", *df.columns), 256))\n",
                "    df=df.withColumn(\"dwStart_Date\",current_timestamp())\n",
                "    df=df.withColumn(\"dwEnd_Date\",lit(None).cast('timestamp'))\n",
                "    return df"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": null,
            "source": [
                "Convert a JSON loaded as string in a JSON field"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "source": [
                "from pyspark.sql.functions import col, from_json, regexp_replace, expr\n",
                "from pyspark.sql.types import MapType, StringType\n",
                "\n",
                "\n",
                "def convertFieldToJSON(df, fieldName,targetField,schema):    \n",
                "    df = df.withColumn(fieldName,  expr(f\"replace({fieldName}, '=', ':')\"))\n",
                "\n",
                "\n",
                "    df = df.withColumn(fieldName, regexp_replace(col(fieldName), r'([{,\\s])(\\w+)(:)',r'$1\"$2\"$3'))\n",
                "\n",
                "    # Step 3: Add double quotes around string values (excluding numbers and lists)\n",
                "    df = df.withColumn(fieldName, regexp_replace(col(fieldName), r':([^\"{\\[\\]\\d][^,}\\]]*)', r':\"$1\"'))\n",
                "\n",
                "    # Step 5: Enclose valid datetime values in quotes (ISO 8601 format)\n",
                "    df = df.withColumn(fieldName, regexp_replace(\n",
                "        col(fieldName),\n",
                "        r'(:)(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.{0,}\\d{0,}\\+\\d{2}:\\d{2})',\n",
                "        r'$1\"$2\"'\n",
                "    ))\n",
                "\n",
                "    dfResult = df.withColumn(targetField, from_json(col(fieldName), schema  ))\n",
                "\n",
                "    return dfResult"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": null,
            "source": [
                "Convert date value to key for dataframe"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "source": [
                "def convertColumnToKey(df,columnName,keyColumnName):\n",
                "    df=df.withColumn(keyColumnName,concat(year(col(columnName)).cast('string'),\n",
                "        lpad(month(col(columnName)).cast('string'),2,'0'),\n",
                "        lpad(dayofmonth(col(columnName)).cast('string'),2,'0')).cast('int'))\n",
                "    return df"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": null,
            "source": [
                "Create Data Frame with SCD Functionality containing both existing and new data"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "source": [
                "def merge_scd_type2(existing_df, new_df,keyColumn):\n",
                "    cols=new_df.columns\n",
                "\n",
                "    new_df = new_df.alias('new')\n",
                "    existing_df = existing_df.alias('existing')\n",
                "\n",
                "    # Perform the join based on row_sha2\n",
                "    new_df = new_df.join(\n",
                "        existing_df, \n",
                "        on='row_sha2', \n",
                "        how='left'\n",
                "    ).filter(\n",
                "        # Keep records where row_sha2 is not present in existing_df (left anti join logic)\n",
                "        col('existing.row_sha2').isNull() | \n",
                "        # Or, where row_sha2 matches and dwEnd_Date is null in new_df and not null in existing_df\n",
                "        ((col('new.dwEnd_Date').isNull()) & (col('existing.dwEnd_Date').isNotNull()))\n",
                "    ).select('new.*')\n",
                "\n",
                "    #if the new record becoming the current one was the current one in multiple moments, this creates a duplication\n",
                "    #the duplication is being removed here\n",
                "    new_df=new_df.dropDuplicates()\n",
                "    \n",
                "    if new_df.rdd.count()==0:\n",
                "       return existing_df\n",
                "    \n",
                "    current_df=existing_df.filter(existing_df.dwEnd_Date.isNull())\n",
                "    merged_df = current_df.alias(\"existing\") \\\n",
                "        .join(new_df.alias(\"new\"), keyColumn, \"outer\") \\\n",
                "        .select(\n",
                "            col('existing.*'),\n",
                "            col(keyColumn).alias(\"NewKey\"),\n",
                "            col(\"new.row_sha2\").alias(\"new_row_hash\"),\n",
                "            col(\"existing.row_sha2\").alias(\"existing_row_hash\"),\n",
                "\n",
                "        )\n",
                "    \n",
                "    # Define the conditions to update existing records\n",
                "    update_condition = (merged_df.NewKey.isNotNull()) & \\\n",
                "                   (merged_df.new_row_hash != merged_df.existing_row_hash)\n",
                "\n",
                "    # Define updated records to insert\n",
                "    columnToJoin=\"new.\" + keyColumn\n",
                "    new_df=new_df.alias(\"new\")    \n",
                "    keysToInsert=merged_df.filter(update_condition).select(\"NewKey\").distinct()\n",
                "    recordsToInsert= new_df.join(keysToInsert,col(columnToJoin)==keysToInsert.NewKey,\"inner\").select(cols).drop(keysToInsert['NewKey'])  \n",
                "\n",
                "    #Define new records\n",
                "    columnToFilter=\"existing.\" + keyColumn\n",
                "    keysToNew=merged_df.filter(merged_df[columnToFilter].isNull()).select(\"NewKey\").distinct()\n",
                "    inserting_df=new_df.join(keysToNew,col(columnToJoin)==keysToNew.NewKey,'right').select(cols).drop(keysToNew[\"NewKey\"])\n",
                "\n",
                "    # Apply the conditions to update existing records\n",
                "    # process explained:\n",
                "    # Identifies a distinct key list of the records which will be inserted\n",
                "    # joins with the existing data\n",
                "    # update the dwEnd_Date on the records which will be inserted\n",
                "    # drop the additional 'newkey' column\n",
                "\n",
                "    keysToUpdate=recordsToInsert.unionByName(inserting_df).select(col(keyColumn).alias('newKey')).distinct()\n",
                "    keysToUpdate=keysToUpdate.alias('keysToUpdate')\n",
                "    existing_df=existing_df.alias('existing_df')\n",
                "    key1name='existing_df.' + keyColumn\n",
                "    key2name='keysToUpdate.newKey'\n",
                "    updatedExisting_df=existing_df.join(keysToUpdate,col(key1name)==col(key2name),'left').select('existing_df.*',col(key2name))\n",
                "\n",
                "\n",
                "    updatedExisting_df = updatedExisting_df.withColumn(\"dwEnd_Date\", \n",
                "        when( (col('newKey')==col(keyColumn)) & (col('dwEnd_Date').isNull()), current_timestamp()).otherwise(updatedExisting_df.dwEnd_Date))\n",
                "\n",
                "    updatedExisting_df=updatedExisting_df.drop('newKey')\n",
                "\n",
                "    oldRecords=updatedExisting_df.filter(updatedExisting_df.dwEnd_Date.isNotNull())\n",
                "\n",
                "    #Eliminate records with updated dwend_Date\n",
                "    # from the current_df dataframe\n",
                "    # otherwise they will be duplicated, coming from current and old dataframes\n",
                "    \n",
                "    current_df=current_df.alias('current_df')\n",
                "    key1name='current_df.' + keyColumn\n",
                "    current_df=current_df.join(keysToUpdate,col(key1name)==col(key2name),'leftanti').select('current_df.*')\n",
                "\n",
                "\n",
                "    #Final union\n",
                "    final_df=current_df.unionByName(recordsToInsert).unionByName(inserting_df).unionByName(oldRecords)\n",
                "\n",
                "    return final_df"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "source": [
                "def fetch_watermark(watermark_table,table_name):\n",
                "    if not mssparkutils.fs.exists(f\"Tables/{watermark_table}\"):\n",
                "        max_time = '1900-01-01'\n",
                "        new_data = spark.createDataFrame([(table_name, max_time)], [\"tableName\", \"lastIngestion\"])\n",
                "        new_data.write.format('delta').mode('overwrite').save(f\"Tables/{watermark_table}\")\n",
                "        return max_time\n",
                "    else:\n",
                "        df = spark.read.format('delta').load(f\"Tables/{watermark_table}\")\n",
                "        row = df.select('lastIngestion').filter(df['TableName'] == table_name ).collect()\n",
                "        if row:\n",
                "            max_time = row[0][0]\n",
                "            return max_time\n",
                "        else:\n",
                "            max_time = '1900-01-01'\n",
                "            return max_time"
            ],
            "outputs": []
        }
    ]
}