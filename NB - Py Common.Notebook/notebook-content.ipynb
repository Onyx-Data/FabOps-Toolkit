{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import sha2, concat_ws,current_timestamp,lit,cast,col, when,concat,lpad,year,month, dayofmonth\n","from datetime import datetime"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2714dcac-c93d-41bc-9bd7-b24ed7883d2e"},{"cell_type":"markdown","source":["Convert Date Value to Key for single value"],"metadata":{},"id":"752d6b88-95eb-4096-a717-e14ef69f394a"},{"cell_type":"code","source":["def convertValueToKey(value):\n","    key= value.strftime('%Y%m%d')\n","    return int(key)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b8f318a3-2e82-4c18-9e49-8dedd1fd805c"},{"cell_type":"markdown","source":["Add SCD fields to existing dataframe"],"metadata":{},"id":"5da26c5e-eecf-43d8-94fc-8391b5f076f4"},{"cell_type":"code","source":["def addSCDFields(df):\n","    df=df.withColumn(\"row_sha2\", sha2(concat_ws(\"||\", *df.columns), 256))\n","    df=df.withColumn(\"dwStart_Date\",current_timestamp())\n","    df=df.withColumn(\"dwEnd_Date\",lit(None).cast('timestamp'))\n","    return df"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d77c37ba-2b9c-4835-a180-f0af0abe80c1"},{"cell_type":"markdown","source":["Convert a JSON loaded as string in a JSON field"],"metadata":{},"id":"20d57ef1-2018-4de8-8e5e-64b26a18e1d4"},{"cell_type":"code","source":["from pyspark.sql.functions import col, from_json, regexp_replace, expr\n","from pyspark.sql.types import MapType, StringType\n","\n","\n","def convertFieldToJSON(df, fieldName,targetField,schema):    \n","    df = df.withColumn(fieldName,  expr(f\"replace({fieldName}, '=', ':')\"))\n","\n","\n","    df = df.withColumn(fieldName, regexp_replace(col(fieldName), r'([{,\\s])(\\w+)(:)',r'$1\"$2\"$3'))\n","\n","    # Step 3: Add double quotes around string values (excluding numbers and lists)\n","    df = df.withColumn(fieldName, regexp_replace(col(fieldName), r':([^\"{\\[\\]\\d][^,}\\]]*)', r':\"$1\"'))\n","\n","    # Step 5: Enclose valid datetime values in quotes (ISO 8601 format)\n","    df = df.withColumn(fieldName, regexp_replace(\n","        col(fieldName),\n","        r'(:)(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.{0,}\\d{0,}\\+\\d{2}:\\d{2})',\n","        r'$1\"$2\"'\n","    ))\n","\n","    dfResult = df.withColumn(targetField, from_json(col(fieldName), schema  ))\n","\n","    return dfResult"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"41aabacd-9c06-487a-ba70-4b0dfec91a0c"},{"cell_type":"markdown","source":["Convert date value to key for dataframe"],"metadata":{},"id":"fe2d5f79-df4b-437b-b5b8-1c48d47bc439"},{"cell_type":"code","source":["def convertColumnToKey(df,columnName,keyColumnName):\n","    df=df.withColumn(keyColumnName,concat(year(col(columnName)).cast('string'),\n","        lpad(month(col(columnName)).cast('string'),2,'0'),\n","        lpad(dayofmonth(col(columnName)).cast('string'),2,'0')).cast('int'))\n","    return df"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78200044-5623-4512-9994-6952c5c1eae9"},{"cell_type":"markdown","source":["Create Data Frame with SCD Functionality containing both existing and new data"],"metadata":{},"id":"5e898447-ddf0-4006-aab8-5055cb679af1"},{"cell_type":"code","source":["def merge_scd_type2(existing_df, new_df,keyColumn):\n","    cols=new_df.columns\n","\n","    new_df = new_df.alias('new')\n","    existing_df = existing_df.alias('existing')\n","\n","    # Perform the join based on row_sha2\n","    new_df = new_df.join(\n","        existing_df, \n","        on='row_sha2', \n","        how='left'\n","    ).filter(\n","        # Keep records where row_sha2 is not present in existing_df (left anti join logic)\n","        col('existing.row_sha2').isNull() | \n","        # Or, where row_sha2 matches and dwEnd_Date is null in new_df and not null in existing_df\n","        ((col('new.dwEnd_Date').isNull()) & (col('existing.dwEnd_Date').isNotNull()))\n","    ).select('new.*')\n","\n","    #if the new record becoming the current one was the current one in multiple moments, this creates a duplication\n","    #the duplication is being removed here\n","    new_df=new_df.dropDuplicates()\n","    \n","    if new_df.rdd.count()==0:\n","       return existing_df\n","    \n","    current_df=existing_df.filter(existing_df.dwEnd_Date.isNull())\n","    merged_df = current_df.alias(\"existing\") \\\n","        .join(new_df.alias(\"new\"), keyColumn, \"outer\") \\\n","        .select(\n","            col('existing.*'),\n","            col(keyColumn).alias(\"NewKey\"),\n","            col(\"new.row_sha2\").alias(\"new_row_hash\"),\n","            col(\"existing.row_sha2\").alias(\"existing_row_hash\"),\n","\n","        )\n","    \n","    # Define the conditions to update existing records\n","    update_condition = (merged_df.NewKey.isNotNull()) & \\\n","                   (merged_df.new_row_hash != merged_df.existing_row_hash)\n","\n","    # Define updated records to insert\n","    columnToJoin=\"new.\" + keyColumn\n","    new_df=new_df.alias(\"new\")    \n","    keysToInsert=merged_df.filter(update_condition).select(\"NewKey\").distinct()\n","    recordsToInsert= new_df.join(keysToInsert,col(columnToJoin)==keysToInsert.NewKey,\"inner\").select(cols).drop(keysToInsert['NewKey'])  \n","\n","    #Define new records\n","    columnToFilter=\"existing.\" + keyColumn\n","    keysToNew=merged_df.filter(merged_df[columnToFilter].isNull()).select(\"NewKey\").distinct()\n","    inserting_df=new_df.join(keysToNew,col(columnToJoin)==keysToNew.NewKey,'right').select(cols).drop(keysToNew[\"NewKey\"])\n","\n","    # Apply the conditions to update existing records\n","    # process explained:\n","    # Identifies a distinct key list of the records which will be inserted\n","    # joins with the existing data\n","    # update the dwEnd_Date on the records which will be inserted\n","    # drop the additional 'newkey' column\n","\n","    keysToUpdate=recordsToInsert.unionByName(inserting_df).select(col(keyColumn).alias('newKey')).distinct()\n","    keysToUpdate=keysToUpdate.alias('keysToUpdate')\n","    existing_df=existing_df.alias('existing_df')\n","    key1name='existing_df.' + keyColumn\n","    key2name='keysToUpdate.newKey'\n","    updatedExisting_df=existing_df.join(keysToUpdate,col(key1name)==col(key2name),'left').select('existing_df.*',col(key2name))\n","\n","\n","    updatedExisting_df = updatedExisting_df.withColumn(\"dwEnd_Date\", \n","        when( (col('newKey')==col(keyColumn)) & (col('dwEnd_Date').isNull()), current_timestamp()).otherwise(updatedExisting_df.dwEnd_Date))\n","\n","    updatedExisting_df=updatedExisting_df.drop('newKey')\n","\n","    oldRecords=updatedExisting_df.filter(updatedExisting_df.dwEnd_Date.isNotNull())\n","\n","    #Eliminate records with updated dwend_Date\n","    # from the current_df dataframe\n","    # otherwise they will be duplicated, coming from current and old dataframes\n","    \n","    current_df=current_df.alias('current_df')\n","    key1name='current_df.' + keyColumn\n","    current_df=current_df.join(keysToUpdate,col(key1name)==col(key2name),'leftanti').select('current_df.*')\n","\n","\n","    #Final union\n","    final_df=current_df.unionByName(recordsToInsert).unionByName(inserting_df).unionByName(oldRecords)\n","\n","    return final_df"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"250e47fc-934b-45c7-90f2-9638b40a2c76"},{"cell_type":"code","source":["def fetch_watermark(watermark_table,table_name):\n","    if not mssparkutils.fs.exists(f\"Tables/{watermark_table}\"):\n","        max_time = '1900-01-01'\n","        new_data = spark.createDataFrame([(table_name, max_time)], [\"tableName\", \"lastIngestion\"])\n","        new_data.write.format('delta').mode('overwrite').save(f\"Tables/{watermark_table}\")\n","        return max_time\n","    else:\n","        df = spark.read.format('delta').load(f\"Tables/{watermark_table}\")\n","        row = df.select('lastIngestion').filter(df['TableName'] == table_name ).collect()\n","        if row:\n","            max_time = row[0][0]\n","            return max_time\n","        else:\n","            max_time = '1900-01-01'\n","            return max_time"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"24877082-29a0-4d8c-b326-809fd21a35aa"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"83037cfc-6791-4ac4-ae26-38075549718d","default_lakehouse_name":"OnyxToolsLake","default_lakehouse_workspace_id":"0409393c-944d-4b64-9222-d4017c7466d0","known_lakehouses":[{"id":"83037cfc-6791-4ac4-ae26-38075549718d"}]}}},"nbformat":4,"nbformat_minor":5}