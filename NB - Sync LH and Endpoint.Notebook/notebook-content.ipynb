{
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "#Users Property\n",
                "SQLendpoint_ID = \"c2d18bf2-0abc-4a70-b4b4-f3c9e7a5c644\" #OnyxToolLake sql endpoint\n",
                "workspace_ID = \"e07d24fb-0ea0-41ff-a228-243d4c3337df\"   #OnyxMetadata-Test"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ],
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "341b00fe-ce60-420d-a7e8-f204f5a0feb4"
        },
        {
            "cell_type": "code",
            "source": [
                "%run NB - Load Configuration"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "f0e7176c-5966-49b6-ad4d-ee68cb8d23dd"
        },
        {
            "cell_type": "code",
            "source": [
                "import json\n",
                "import time\n",
                "import struct\n",
                "import sqlalchemy\n",
                "import pyodbc\n",
                "import notebookutils\n",
                "import pandas as pd\n",
                "from pyspark.sql import functions as fn\n",
                "from pyspark.sql.functions import current_timestamp, expr\n",
                "from datetime import datetime\n",
                "import sempy.fabric as fabric\n",
                "from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "f0d44e3b-641f-4a24-b286-763b250f703e"
        },
        {
            "cell_type": "code",
            "source": [
                "SQLEndpointsRefreshCalls=\"SQLEndpointsRefreshCalls\""
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "598ba138-9b36-4537-bd31-8ae6bfc11468"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Create the Rest Client"
            ],
            "metadata": {},
            "id": "5d08912e-865b-4b7c-ad4e-cab75d9659a9"
        },
        {
            "cell_type": "code",
            "source": [
                "#Instantiate the client\n",
                "client = fabric.FabricRestClient()"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "88380f48-3f3d-4372-a993-aa66ca439b8a"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Check for a previous batch Id on the same SQL Endpoint Id"
            ],
            "metadata": {},
            "id": "9f7bd6c1-8eaa-48d7-b795-0cc1bb57bb7f"
        },
        {
            "cell_type": "code",
            "source": [
                "batches=spark.read.format(\"delta\").load(f'Tables/{SQLEndpointsRefreshCalls}')\n",
                "\n",
                "batches = batches.filter(\n",
                "    (batches.sqlEndpointId == SQLendpoint_ID) & \n",
                "    (batches.executionTime >= expr(\"current_timestamp() - interval 30 minutes\"))\n",
                ")\n",
                "\n",
                "top_record = batches.orderBy(batches.executionTime.desc()).limit(1)\n",
                "\n",
                "# if there is a batch, we will test the most recent one to confirm it's completed\n",
                "if top_record.count() > 0:\n",
                "    batch_id_value = top_record.collect()[0][\"batchId\"]\n",
                "    # URL so we can get the status of the sync\n",
                "    statusuri = f\"/v1.0/myorg/lhdatamarts/{SQLendpoint_ID}/batches/{batch_id_value}\"\n",
                "    # turn response into object\n",
                "    statusresponsedata = client.get(statusuri).json()\n",
                "    # get the status of the check\n",
                "    progressState = statusresponsedata[\"progressState\"]\n",
                "    if (progressState==\"inProgress\"):\n",
                "        mssparkutils.notebook.exit(0)\n",
                "    else:\n",
                "        print(progressState)\n",
                "else:\n",
                "    print('no batch id record')\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "8209aa03-eeef-467b-998e-e9ee7c1315ae"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### It's ok to make a refresh request"
            ],
            "metadata": {},
            "id": "1a11e192-6828-4e5d-9c88-0a36fa36080b"
        },
        {
            "cell_type": "code",
            "source": [
                "  \n",
                "# URI for the call\n",
                "uri = f\"/v1.0/myorg/lhdatamarts/{SQLendpoint_ID}\"\n",
                "# This is the action, we want to take\n",
                "payload = {\"commands\":[{\"$type\":\"MetadataRefreshCommand\"}]}\n",
                " \n",
                " \n",
                "# Call the REST API\n",
                "response = client.post(uri,json= payload)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "74c637cf-0a37-4777-a596-cf87a810b627"
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Save the BatchId"
            ],
            "metadata": {},
            "id": "05e51179-eda9-4900-b032-15316f4e505d"
        },
        {
            "cell_type": "code",
            "source": [
                "\n",
                "# return the response from json into an object we can get values from\n",
                "data = json.loads(response.text)\n",
                "\n",
                "# We just need this, we pass this to call to check the status\n",
                "batchId = data[\"batchId\"]\n",
                "\n",
                "# Create a DataFrame for the new record\n",
                "new_record = [(batchId, SQLendpoint_ID,  datetime.now())]\n",
                "new_record_df = spark.createDataFrame(new_record, batches.schema)\n",
                "\n",
                "display(new_record_df)\n",
                "\n",
                "# Append the new record to the Delta table\n",
                "new_record_df.write.format(\"delta\").mode(\"append\").save(f'Tables/{SQLEndpointsRefreshCalls}')"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "2152c296-c067-40c8-b75b-54bf59d1cc88"
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "a365ComputeOptions": null,
        "sessionKeepAliveTimeout": 0,
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "dependencies": {
            "lakehouse": {
                "default_lakehouse": "83037cfc-6791-4ac4-ae26-38075549718d",
                "default_lakehouse_name": "OnyxToolsLake",
                "default_lakehouse_workspace_id": "0409393c-944d-4b64-9222-d4017c7466d0",
                "known_lakehouses": [
                    {
                        "id": "83037cfc-6791-4ac4-ae26-38075549718d"
                    }
                ]
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}