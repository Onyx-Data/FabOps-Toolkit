{"cells":[{"cell_type":"markdown","source":["#### Workspace and Lakehouse Utility Functions\n","\n","This section contains utility functions to resolve workspace and lakehouse IDs using semantic-link (sempy) library."],"metadata":{},"id":"ae91aae0-ba61-4819-a425-ed53600a6080"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","SqlEndpoint_id = fabric.resolve_item_id(lakehouse_name, 'SqlEndpoint', workspace_id)\n","\n","print(f\"Current workspace ID: {workspace_id}\")\n","print(f\"Current lakehouse ID: {lakehouse_id}\")\n","print(f\"Current SqlEndpoint ID: {SqlEndpoint_id}\")\n"],"outputs":[],"execution_count":null,"metadata":{"run_control":{"frozen":false},"editable":true,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1366ce75-51a0-423c-b00e-744c07c426bd"},{"cell_type":"code","source":["from pyspark.sql.functions import col, lit, current_timestamp\n","from datetime import datetime\n","import uuid\n","\n","\n","maintenance_folder = \"Files/Maintenance/TablesMaintenance/\"\n","current_date = datetime.now().strftime('%Y-%m-%d')\n","maintenance_date = datetime.now().strftime('%Y%m%d')\n","maintenance_date_folder = f\"maintenancedate_{current_date}\"\n","distinct_folder = str(uuid.uuid4())[:8]\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"run_control":{"frozen":false},"editable":true,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21253188-4d12-42dd-90e2-200fa9baca67"},{"cell_type":"markdown","source":["#### Performance"],"metadata":{},"id":"a3ab0da9-3bab-4dbf-b346-d7888a54a48c"},{"cell_type":"code","source":["%%spark\n","val specificpath=\"PerfBaseline\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"164fa6c4-9154-46da-bab6-8effe653cf11"},{"cell_type":"code","source":["%%pyspark\n","\n","#make configurations available in scala\n","\n","spark.conf.set(\"SqlEndpointID\",SqlEndpoint_id)\n","spark.conf.set(\"workspaceID\",workspace_id)\n","spark.conf.set(\"maintenanceFolder\",maintenance_folder)\n","spark.conf.set(\"maintenanceDateFolder\",maintenance_date_folder)\n","spark.conf.set(\"lakehouseName\",lakehouse_name)\n","spark.conf.set(\"distinctFolder\",distinct_folder)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b9943689-6771-4b8a-a847-9cd05260da8f"},{"cell_type":"code","source":["%%spark\n","// load config values in scala\n","\n","val SqlEndpointID = spark.conf.get(\"SqlEndpointID\")\n","val workspaceID = spark.conf.get(\"workspaceID\")\n","val maintenanceFolder = spark.conf.get(\"maintenanceFolder\")\n","val maintenanceDateFolder = spark.conf.get(\"maintenanceDateFolder\")\n","val lakehouseName = spark.conf.get(\"lakehouseName\")\n","val distinctFolder = spark.conf.get(\"distinctFolder\")\n","\n","val fullpath = s\"$maintenanceFolder$maintenanceDateFolder/$specificpath/$distinctFolder\"\n","println(s\"workspaceID: $workspaceID\")\n","println(s\"Performance data will be saved to: $fullpath\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"13b08730-9a8e-404c-9a3e-1ec6f2842c42"},{"cell_type":"code","source":["%%spark\n","import java.time.LocalDate\n","import java.time.format.DateTimeFormatter\n","\n","// Subtract 7 days from the current date\n","val dateSevenDaysAgo = LocalDate.now().minusDays(7)\n","\n","val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n","val dateString = dateSevenDaysAgo.format(formatter)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"e6c4a2db-6ce8-48df-b222-0afb3f90871a"},{"cell_type":"code","source":["%%spark\n","val sqlprepare1=s\"\"\"\"with qry as (\n","    select distributed_statement_id, program_name, data_scanned_disk_mb, data_scanned_memory_mb,data_scanned_remote_storage_mb,\n","    REPLACE(REPLACE(command, CHAR(13), ''), CHAR(10), '') as command,\n","    total_elapsed_time_ms, start_time, end_time,allocated_cpu_time_ms, status,row_count,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.DatasetId') \n","            ELSE NULL \n","        END AS DatasetId,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.Sources[0].ReportId') \n","            ELSE NULL \n","        END AS ReportId,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.Sources[0].VisualId') \n","            ELSE NULL \n","        END AS VisualId, label\n","    from queryinsights.exec_requests_history\n","    where program_name in ('Core .Net SqlClient Data Provider','.Net SqlClient Data Provider','Framework Microsoft SqlClient Data Provider','PowerBIPremium-DirectQuery','Microsoft JDBC Driver for SQL Server')\n","    and start_time > '$dateString' and command not like '%sys.sp_set_session_context%' and status='Succeeded' \n","),\n","num as (\n","    select *, row_number() over (partition by DatasetId, ReportId order by total_elapsed_time_ms DESC) as rownum\n","    from qry\n",")\n","\"\"\"\"\n","\n","val sqlprepare2=s\"\"\"\"with qry2 as (\n","    select distributed_statement_id,program_name,data_scanned_disk_mb,data_scanned_memory_mb,data_scanned_remote_storage_mb,\n","    REPLACE(REPLACE(command, CHAR(13), ''), CHAR(10), '') as command,\n","    total_elapsed_time_ms, start_time, end_time,allocated_cpu_time_ms, status,row_count,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.DatasetId') \n","            ELSE NULL \n","        END AS DatasetId,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.Sources[0].ReportId') \n","            ELSE NULL \n","        END AS ReportId,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.Sources[0].VisualId') \n","            ELSE NULL \n","        END AS VisualId, label\n","    from queryinsights.exec_requests_history\n","    where program_name in ('Core .Net SqlClient Data Provider','.Net SqlClient Data Provider','Framework Microsoft SqlClient Data Provider','PowerBIPremium-DirectQuery','Microsoft JDBC Driver for SQL Server')\n","    and start_time > '$dateString' and command not like '%sys.sp_set_session_context%' and status='Succeeded' \n","),\n","num as (\n","    select *, row_number() over (partition by DatasetId, ReportId order by data_scanned_memory_mb DESC) as rownum\n","    from qry2\n",") \n","\"\"\"\"\n","\n","val sqlprepare3=s\"\"\"\"with qry3 as (\n","    select distributed_statement_id,program_name,data_scanned_disk_mb,data_scanned_memory_mb,data_scanned_remote_storage_mb,\n","    REPLACE(REPLACE(command, CHAR(13), ''), CHAR(10), '') as command,\n","    total_elapsed_time_ms, start_time, end_time,allocated_cpu_time_ms, status,row_count,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.DatasetId') \n","            ELSE NULL \n","        END AS DatasetId,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.Sources[0].ReportId') \n","            ELSE NULL \n","        END AS ReportId,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.Sources[0].VisualId') \n","            ELSE NULL \n","        END AS VisualId, label\n","    from queryinsights.exec_requests_history\n","    where program_name in ('Core .Net SqlClient Data Provider','.Net SqlClient Data Provider','Framework Microsoft SqlClient Data Provider','PowerBIPremium-DirectQuery','Microsoft JDBC Driver for SQL Server')\n","    and start_time > '$dateString' and command not like '%sys.sp_set_session_context%' and status='Succeeded' \n","),\n","num as (\n","    select *, row_number() over (partition by DatasetId, ReportId order by data_scanned_remote_storage_mb DESC) as rownum\n","    from qry3\n",") \n","\"\"\"\"\n","\n","val sqlprepare4=s\"\"\"\"with qry4 as (\n","    select distributed_statement_id,program_name,data_scanned_disk_mb,data_scanned_memory_mb,data_scanned_remote_storage_mb,\n","    REPLACE(REPLACE(command, CHAR(13), ''), CHAR(10), '') as command,\n","    total_elapsed_time_ms, start_time, end_time,allocated_cpu_time_ms, status,row_count,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.DatasetId') \n","            ELSE NULL \n","        END AS DatasetId,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.Sources[0].ReportId') \n","            ELSE NULL \n","        END AS ReportId,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.Sources[0].VisualId') \n","            ELSE NULL \n","        END AS VisualId, label\n","    from queryinsights.exec_requests_history\n","    where program_name in ('Core .Net SqlClient Data Provider','.Net SqlClient Data Provider','Framework Microsoft SqlClient Data Provider','PowerBIPremium-DirectQuery','Microsoft JDBC Driver for SQL Server')\n","    and start_time > '$dateString' and command not like '%sys.sp_set_session_context%' and status='Succeeded' \n","),\n","num as (\n","    select *, row_number() over (partition by DatasetId, ReportId order by allocated_cpu_time_ms DESC) as rownum\n","    from qry4\n",") \n","\"\"\"\"\n","\n","val sqlprepare5=s\"\"\"\"with qry5 as (\n","    select distributed_statement_id,program_name,data_scanned_disk_mb,data_scanned_memory_mb,data_scanned_remote_storage_mb,\n","    REPLACE(REPLACE(command, CHAR(13), ''), CHAR(10), '') as command,\n","    total_elapsed_time_ms, start_time, end_time,allocated_cpu_time_ms, status,row_count,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.DatasetId') \n","            ELSE NULL \n","        END AS DatasetId,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.Sources[0].ReportId') \n","            ELSE NULL \n","        END AS ReportId,\n","        CASE \n","            WHEN ISJSON(label) = 1 THEN JSON_VALUE(label, '$$.Sources[0].VisualId') \n","            ELSE NULL \n","        END AS VisualId, label\n","    from queryinsights.exec_requests_history\n","    where program_name in ('Core .Net SqlClient Data Provider','.Net SqlClient Data Provider','Framework Microsoft SqlClient Data Provider','PowerBIPremium-DirectQuery','Microsoft JDBC Driver for SQL Server')\n","    and start_time > '$dateString' and command not like '%sys.sp_set_session_context%' and status='Succeeded' \n","),\n","num as (\n","    select *, row_number() over (partition by DatasetId, ReportId order by row_count DESC) as rownum\n","    from qry5\n",") \n","\"\"\"\n","\n","val mainquery=\"select * from num where rownum <21\"\n","\n","case class Query(name: String, query: String)\n","\n","\n","val queries = Seq(\n","  Query(\"TopEllapsedTime\", sqlprepare1),\n","  Query(\"TopScannedMemory\", sqlprepare2),\n","  Query(\"TopRemoteStorage\", sqlprepare3),\n","  Query(\"TopAllocatedCPU\", sqlprepare4),\n","  Query(\"TopRowCount\", sqlprepare5)\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"1e474744-d107-4467-b472-0950aa3f4744"},{"cell_type":"code","source":["%%pyspark\n","\n","lh_name = lakehouse_name \n","workspaceID= workspace_id\n","\n","spark.conf.set(\"lh_name\",lh_name)\n","spark.conf.set(\"workspaceID\",workspace_id)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70881983-c003-40a7-b175-45c79eaadde0"},{"cell_type":"code","source":["%%spark\n","import com.microsoft.spark.fabric.tds.implicits.read.FabricSparkTDSImplicits._\n","import com.microsoft.spark.fabric.Constants\n","\n","val lh_name = spark.conf.get(\"lh_name\")\n","val workspaceID = spark.conf.get(\"workspaceID\")\n","\n","queries.foreach { qry =>\n","    val query = qry.query.trim.stripPrefix(\"\\\"\").stripSuffix(\"\\\"\")\n","    println(s\" Running Query: $query\")\n","    \n","    val df = spark.read\n","      .option(Constants.WorkspaceId, workspaceID)\n","      .option(Constants.DatabaseName, lh_name)\n","      .option(\"prepareQuery\", query)\n","      .synapsesql(mainquery)\n","\n","    val fname = qry.name\n","    val finalfullpath = fullpath + s\"/$fname\"\n","    \n","    df.write.format(\"parquet\").save(finalfullpath)\n","    println(s\"✅ Saved to: $finalfullpath\")\n","}\n","\n","\n","println(\"\\n🎯 Process Completed.\")\n","println(s\"🔗 Workspace ID: $workspaceID\")\n","println(s\"🏠 Lakehouse Name: $lh_name\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"scala","language_group":"synapse_pyspark"}},"id":"1dbacf4b-b172-4bd8-8741-69567606ec38"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":null,"default_lakehouse_name":"","default_lakehouse_workspace_id":""}}},"nbformat":4,"nbformat_minor":5}