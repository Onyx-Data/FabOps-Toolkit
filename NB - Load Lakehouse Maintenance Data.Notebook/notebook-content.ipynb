{"cells":[{"cell_type":"markdown","source":["#### Load TableSizeAnalysis data"],"metadata":{},"id":"9a93ac31-b120-40f1-ab49-cbe86835a15c"},{"cell_type":"code","source":["from datetime import datetime\n","from pyspark.sql.functions import lit, col\n","\n","# Define the starting date\n","start_date = datetime.strptime(\"2024-12-26\", \"%Y-%m-%d\")\n","\n","# Define the base path of the Lakehouse\n","base_path = \"Files/Maintenance/TablesMaintenance\"\n","\n","def process_data(base_path, time_period, start_date):\n","    final_combined_df = None\n","\n","    try:\n","        maintenance_date_folders = mssparkutils.fs.ls(base_path)\n","        for folder in maintenance_date_folders:\n","            folder_name = folder.name\n","\n","            # Check if the folder name starts with \"maintenancedate_\"\n","            if folder_name.startswith(\"maintenancedate_\"):\n","                # Extract the date part from the folder name\n","                folder_date_str = folder_name.replace(\"maintenancedate_\", \"\").strip(\"/\")\n","                folder_date = datetime.strptime(folder_date_str, \"%Y-%m-%d\")\n","\n","                # Process only folders with dates on or after the start date\n","                if folder_date >= start_date:\n","                    maintenance_date_path = folder.path\n","\n","                    # Append 'TableAnalysis' to the maintenance_date_path\n","                    table_analysis_path = f\"{maintenance_date_path}/TableAnalysis\"\n","\n","                    # List all subfolders under the TableAnalysis folder\n","                    try:\n","                        subfolders = mssparkutils.fs.ls(table_analysis_path)\n","                        for subfolder in subfolders:\n","                            subfolder_path = subfolder.path\n","\n","                            # Path to the \"tablesSize\" folder for the given time period\n","                            tables_fullanalysis_path = f\"{subfolder_path}/{time_period}/tablesSize\"\n","\n","                            # Check if the path exists\n","                            if mssparkutils.fs.exists(tables_fullanalysis_path):\n","                                try:\n","                                    # Read the parquet files from the folder\n","                                    df = spark.read.parquet(tables_fullanalysis_path)\n","\n","                                    # Add the maintenance date as a new column\n","                                    df = df.withColumn(\"MaintenanceDate\", lit(folder_date_str).cast(\"date\"))\n","\n","                                    # Combine data from all folders\n","                                    if final_combined_df is None:\n","                                        final_combined_df = df\n","                                    else:\n","                                        final_combined_df = final_combined_df.union(df)\n","\n","                                    print(f\"Successfully read data from {tables_fullanalysis_path}\")\n","                                except Exception as e:\n","                                    print(f\"Failed to read data from {tables_fullanalysis_path}: {e}\")\n","                            else:\n","                                print(f\"Path does not exist: {tables_fullanalysis_path}. Skipping...\")\n","                    except Exception as e:\n","                        print(f\"Failed to list subfolders in {table_analysis_path}: {e}\")\n","    except Exception as e:\n","        print(f\"Failed to list maintenancedate_* subfolders in {base_path}: {e}\")\n","\n","    if final_combined_df is not None:\n","        final_combined_df = final_combined_df.withColumn(\"MaintenanceTime\", lit(time_period.capitalize()))\n","\n","    return final_combined_df\n","\n","# Process data for \"before\" and \"after\"\n","before_final_combined_df = process_data(base_path, \"before\", start_date)\n","after_final_combined_df = process_data(base_path, \"after\", start_date)\n","\n","# Combine both DataFrames\n","df_tableSize = None\n","if before_final_combined_df and after_final_combined_df:\n","    df_tableSize = before_final_combined_df.union(after_final_combined_df)\n","elif before_final_combined_df:\n","    df_tableSize = before_final_combined_df\n","elif after_final_combined_df:\n","    df_tableSize = after_final_combined_df\n","\n","# Write the combined DataFrame back to the Delta table\n","delta_table_path = \"Tables/Maintenance_tablesSize\"\n","if df_tableSize is not None:\n","    df_tableSize.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n","    print(f\"Data successfully loaded into Delta table at {delta_table_path}\")\n","else:\n","    print(\"No data was loaded. Check the paths and files in the Lakehouse.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"bd6ccf15-ba85-4fd2-bb04-57318b29a052","normalized_state":"finished","queued_time":"2025-07-15T05:22:33.4368083Z","session_start_time":null,"execution_start_time":"2025-07-15T05:22:39.3739414Z","execution_finish_time":"2025-07-15T05:22:50.1488769Z","parent_msg_id":"cceb0afc-a5ed-47af-8250-301eb5752604"},"text/plain":"StatementMeta(, bd6ccf15-ba85-4fd2-bb04-57318b29a052, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Successfully read data from abfss://0409393c-944d-4b64-9222-d4017c7466d0@onelake.dfs.fabric.microsoft.com/83037cfc-6791-4ac4-ae26-38075549718d/Files/Maintenance/TablesMaintenance/maintenancedate_2025-07-14/TableAnalysis/e854310e14a84c4aae15d3dff7412f90/before/tablesSize\nSuccessfully read data from abfss://0409393c-944d-4b64-9222-d4017c7466d0@onelake.dfs.fabric.microsoft.com/83037cfc-6791-4ac4-ae26-38075549718d/Files/Maintenance/TablesMaintenance/maintenancedate_2025-07-14/TableAnalysis/e854310e14a84c4aae15d3dff7412f90/after/tablesSize\n"]},{"output_type":"stream","name":"stdout","text":["Data successfully loaded into Delta table at Tables/Maintenance_tablesSize\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"15cfea7e-41e7-4b84-8622-d858fbfbdc01"},{"cell_type":"markdown","source":["#### Load Performance Baseline Data"],"metadata":{},"id":"df0a7ae5-a733-4e46-ac36-b769cc229a0c"},{"cell_type":"code","source":["from datetime import datetime\n","from pyspark.sql.functions import lit, col\n","from pyspark.sql import DataFrame\n","\n","# Define the base path of the Lakehouse\n","base_path = \"Files/Maintenance/TablesMaintenance\"\n","\n","# Define the starting date\n","start_date = datetime.strptime(\"2025-01-21\", \"%Y-%m-%d\")\n","\n","# Helper function to read and process parquet files\n","def process_folder(base_path: str, subfolder_name: str, performance_type: str, start_date: datetime) -> DataFrame:\n","    df_result = None\n","    try:\n","        maintenance_date_folders = mssparkutils.fs.ls(base_path)\n","        for folder in maintenance_date_folders:\n","            folder_name = folder.name\n","            # Check if the folder name starts with \"maintenancedate_\"\n","            if folder_name.startswith(\"maintenancedate_\"):\n","                folder_date_str = folder_name.replace(\"maintenancedate_\", \"\").strip(\"/\")\n","                folder_date = datetime.strptime(folder_date_str, \"%Y-%m-%d\")\n","\n","                # Process only folders with dates on or after the start date\n","                if folder_date >= start_date:\n","                    maintenance_date_path = folder.path\n","                    perf_baseline_path = f\"{maintenance_date_path}/PerfBaseline\"\n","\n","                    try:\n","                        guid_folders = mssparkutils.fs.ls(perf_baseline_path)\n","                        for guid_folder in guid_folders:\n","                            guid_path = guid_folder.path\n","\n","                            # Path to the performance metric folder (no 'before' folder anymore)\n","                            target_path = f\"{guid_path}/{subfolder_name}\"\n","\n","                            # Check if the target path exists\n","                            if mssparkutils.fs.exists(target_path):\n","                                try:\n","                                    # Read the parquet files\n","                                    df_temp = spark.read.parquet(target_path)\n","                                    df_temp = (\n","                                        df_temp\n","                                        .withColumn(\"MaintenanceDate\", lit(folder_date_str).cast(\"date\"))\n","                                        .withColumn(\"PerformanceType\", lit(performance_type))\n","                                    )\n","\n","                                    # Append or union the DataFrame\n","                                    df_result = df_temp if df_result is None else df_result.union(df_temp)\n","                                    print(f\"Successfully read data from {target_path}\")\n","                                except Exception as e:\n","                                    print(f\"Failed to read data from {target_path}: {e}\")\n","                            else:\n","                                print(f\"Path does not exist: {target_path}. Skipping...\")\n","                    except Exception as e:\n","                        print(f\"Failed to list GUID folders in {perf_baseline_path}: {e}\")\n","    except Exception as e:\n","        print(f\"Failed to list maintenancedate_* subfolders in {base_path}: {e}\")\n","    return df_result\n","\n","# Process each performance type\n","df_AllocatedCPU = process_folder(base_path, \"TopAllocatedCPU\", \"Allocated CPU\", start_date)\n","df_EllapsedTime = process_folder(base_path, \"TopEllapsedTime\", \"EllapsedTime\", start_date)\n","df_RemoteStorage = process_folder(base_path, \"TopRemoteStorage\", \"RemoteStorage\", start_date)\n","df_RowCount = process_folder(base_path, \"TopRowCount\", \"RowCount\", start_date)\n","df_ScannedMemory = process_folder(base_path, \"TopScannedMemory\", \"ScannedMemory\", start_date)\n","\n","# Combine all performance types into a single DataFrame\n","df_Maintenance_Performance = (\n","    df_AllocatedCPU\n","    .union(df_EllapsedTime)\n","    .union(df_RemoteStorage)\n","    .union(df_RowCount)\n","    .union(df_ScannedMemory)\n",")\n","df_Maintenance_Performance = df_Maintenance_Performance.drop(\"label\", \"rownum\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"bd6ccf15-ba85-4fd2-bb04-57318b29a052","normalized_state":"finished","queued_time":"2025-07-15T05:22:33.6719697Z","session_start_time":null,"execution_start_time":"2025-07-15T05:22:50.1514884Z","execution_finish_time":"2025-07-15T05:22:52.5173808Z","parent_msg_id":"15b10b04-265f-4edb-bc3f-d8ac979a5845"},"text/plain":"StatementMeta(, bd6ccf15-ba85-4fd2-bb04-57318b29a052, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Successfully read data from abfss://0409393c-944d-4b64-9222-d4017c7466d0@onelake.dfs.fabric.microsoft.com/83037cfc-6791-4ac4-ae26-38075549718d/Files/Maintenance/TablesMaintenance/maintenancedate_2025-07-14/PerfBaseline/9959b459/TopAllocatedCPU\nSuccessfully read data from abfss://0409393c-944d-4b64-9222-d4017c7466d0@onelake.dfs.fabric.microsoft.com/83037cfc-6791-4ac4-ae26-38075549718d/Files/Maintenance/TablesMaintenance/maintenancedate_2025-07-14/PerfBaseline/9959b459/TopEllapsedTime\n"]},{"output_type":"stream","name":"stdout","text":["Successfully read data from abfss://0409393c-944d-4b64-9222-d4017c7466d0@onelake.dfs.fabric.microsoft.com/83037cfc-6791-4ac4-ae26-38075549718d/Files/Maintenance/TablesMaintenance/maintenancedate_2025-07-14/PerfBaseline/9959b459/TopRemoteStorage\nSuccessfully read data from abfss://0409393c-944d-4b64-9222-d4017c7466d0@onelake.dfs.fabric.microsoft.com/83037cfc-6791-4ac4-ae26-38075549718d/Files/Maintenance/TablesMaintenance/maintenancedate_2025-07-14/PerfBaseline/9959b459/TopRowCount\nSuccessfully read data from abfss://0409393c-944d-4b64-9222-d4017c7466d0@onelake.dfs.fabric.microsoft.com/83037cfc-6791-4ac4-ae26-38075549718d/Files/Maintenance/TablesMaintenance/maintenancedate_2025-07-14/PerfBaseline/9959b459/TopScannedMemory\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a6c11b74-6dc2-4616-9f48-3b5d06be1e27"},{"cell_type":"markdown","source":["### **Translate Dataset name and Report name**"],"metadata":{},"id":"e6eab7b8-a739-48f6-b5ed-162dc2b9e971"},{"cell_type":"markdown","source":["#### Specify the workspace(s) to the Dataset name and Report Name. \n","For example:\n","#####  List of workspaces \n","workspaces = [\"workspace 1\", \"workspace 2\"]   # Add many as required\n"],"metadata":{},"id":"a9598ca6-7767-4ca4-83f5-e193404795d9"},{"cell_type":"code","source":["#  List of workspaces \n","workspaces = [\"OnyxTools-Test\"]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"bd6ccf15-ba85-4fd2-bb04-57318b29a052","normalized_state":"finished","queued_time":"2025-07-15T05:22:33.6737696Z","session_start_time":null,"execution_start_time":"2025-07-15T05:22:52.520069Z","execution_finish_time":"2025-07-15T05:22:52.7889366Z","parent_msg_id":"3d2a342b-3d37-435e-ab14-5deed86628db"},"text/plain":"StatementMeta(, bd6ccf15-ba85-4fd2-bb04-57318b29a052, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"67a212e7-38fe-41b5-a673-149cbba1df30"},{"cell_type":"code","source":["# Translate Dataset Names\n","\n","import sempy.fabric as fabric\n","from pyspark.sql.functions import lit\n","# Function to generate a DataFrame for a given workspace\n","def create_dataset_dataframe(workspace_name):\n","    dataset_data = fabric.list_datasets(workspace_name)\n","    return (\n","        spark.createDataFrame(dataset_data)\n","        .withColumnRenamed(\"Dataset ID\", \"DatasetID\")\n","        .withColumnRenamed(\"Dataset Name\", \"DatasetName\")\n","        .withColumn(\"Dataset_Workspace\", lit(workspace_name))\n","    )\n","\n","# Generate and union all report DataFrames\n","dataset_Name_df = None\n","for workspace in workspaces:\n","    workspace_df = create_dataset_dataframe(workspace)\n","    dataset_Name_df = workspace_df if dataset_Name_df is None else dataset_Name_df.union(workspace_df)\n","    # display(dataset_Name_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"bd6ccf15-ba85-4fd2-bb04-57318b29a052","normalized_state":"finished","queued_time":"2025-07-15T05:22:33.7560254Z","session_start_time":null,"execution_start_time":"2025-07-15T05:22:52.7908938Z","execution_finish_time":"2025-07-15T05:23:15.3747862Z","parent_msg_id":"345ca143-cf93-4055-9588-458a92b96732"},"text/plain":"StatementMeta(, bd6ccf15-ba85-4fd2-bb04-57318b29a052, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"967bd893-fbcd-459f-8af2-d6cd29a85f61"},{"cell_type":"code","source":["# Translate Report Names\n","\n","import sempy.fabric as fabric\n","from pyspark.sql.functions import lit\n","\n","# Function to generate a DataFrame for a given workspace\n","def create_report_dataframe(workspace_name):\n","    report_data = fabric.list_reports(workspace_name)\n","    return (\n","        spark.createDataFrame(report_data)\n","        .withColumnRenamed(\"Id\", \"ReportId\")\n","        .withColumnRenamed(\"Name\", \"Report_Name\")\n","        .withColumn(\"Report_Workspace\", lit(workspace_name))\n","    )\n","\n","# Generate and union all report DataFrames\n","report_name_df = None\n","for workspace in workspaces:\n","    workspace_df = create_report_dataframe(workspace)\n","    report_name_df = workspace_df if report_name_df is None else report_name_df.union(workspace_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"bd6ccf15-ba85-4fd2-bb04-57318b29a052","normalized_state":"finished","queued_time":"2025-07-15T05:22:34.002913Z","session_start_time":null,"execution_start_time":"2025-07-15T05:23:15.3768648Z","execution_finish_time":"2025-07-15T05:23:16.2580937Z","parent_msg_id":"e3858903-b22d-44cf-a1ad-8b50e9a1880b"},"text/plain":"StatementMeta(, bd6ccf15-ba85-4fd2-bb04-57318b29a052, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2d5ee847-4139-4b3f-85bb-765dd897d3b4"},{"cell_type":"markdown","source":["### Joining Performance Data to Report Name and Dataset name"],"metadata":{},"id":"546b3f31-407c-4155-bc29-3b47c6a894a5"},{"cell_type":"code","source":["# Join with dataset DataFrame \n","dataset_df = (\n","    df_Maintenance_Performance.join(\n","        dataset_Name_df.select(\"DatasetId\", \"DatasetName\"), \n","        on=\"DatasetId\", \n","        how=\"left\"\n","    )\n","    .select(df_Maintenance_Performance[\"*\"], dataset_Name_df[\"DatasetName\"])\n",")\n","\n","# Final join with report DataFrame\n","final_df = (\n","    dataset_df.join(\n","        report_name_df.select(\"ReportId\", \"Report_Name\", \"Report_Workspace\"),\n","        on=\"ReportId\",\n","        how=\"left\"\n","    )\n","    .select(\n","        dataset_df[\"*\"],\n","        report_name_df[\"Report_Name\"],\n","        report_name_df[\"Report_Workspace\"]\n","    )\n",")\n","\n","#display(final_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"bd6ccf15-ba85-4fd2-bb04-57318b29a052","normalized_state":"finished","queued_time":"2025-07-15T05:22:34.1366772Z","session_start_time":null,"execution_start_time":"2025-07-15T05:23:16.2603113Z","execution_finish_time":"2025-07-15T05:23:17.0681612Z","parent_msg_id":"4dbb5a18-a3f5-4463-a16c-d0786dd8a3df"},"text/plain":"StatementMeta(, bd6ccf15-ba85-4fd2-bb04-57318b29a052, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"763063fb-be13-4d9c-9f1f-0b6df701cfa0"},{"cell_type":"code","source":["# Write the combined DataFrame back to the Delta table\n","delta_table_path = \"Tables/Maintenance_Performance\"\n","\n","if final_df is not None:\n","    final_df.write \\\n","        .format(\"delta\") \\\n","        .mode(\"overwrite\") \\\n","        .option(\"overwriteSchema\", \"true\") \\\n","        .save(delta_table_path)\n","    print(f\"Data successfully loaded into Delta table at {delta_table_path}\")\n","else:\n","    print(\"No data was loaded. Check the paths and files in the Lakehouse.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"bd6ccf15-ba85-4fd2-bb04-57318b29a052","normalized_state":"finished","queued_time":"2025-07-15T05:22:34.215938Z","session_start_time":null,"execution_start_time":"2025-07-15T05:23:17.0702153Z","execution_finish_time":"2025-07-15T05:23:23.1668819Z","parent_msg_id":"22d09dd6-cc85-46ba-a2a2-b5c33c67508b"},"text/plain":"StatementMeta(, bd6ccf15-ba85-4fd2-bb04-57318b29a052, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Data successfully loaded into Delta table at Tables/Maintenance_Performance\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9dd77c52-c6f8-4d92-9321-efe38eafa4a4"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"83037cfc-6791-4ac4-ae26-38075549718d","default_lakehouse_name":"OnyxToolsLake","default_lakehouse_workspace_id":"0409393c-944d-4b64-9222-d4017c7466d0","known_lakehouses":[{"id":"83037cfc-6791-4ac4-ae26-38075549718d"}]}}},"nbformat":4,"nbformat_minor":5}