{
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "%run NB - Load Configuration"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "c9e84448-2666-48b7-a6e2-7d8472aa76a9"
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Load TableSizeAnalysis data"
            ],
            "metadata": {},
            "id": "5a08af73-49cb-4263-8836-afc281493704"
        },
        {
            "cell_type": "code",
            "source": [
                "from datetime import datetime\n",
                "from pyspark.sql.functions import lit, col\n",
                "\n",
                "# Define the starting date\n",
                "start_date = datetime.strptime(\"2024-12-26\", \"%Y-%m-%d\")\n",
                "\n",
                "# Define the base path of the Lakehouse\n",
                "base_path = \"Files/Maintenance/TablesMaintenance\"\n",
                "\n",
                "def process_data(base_path, time_period, start_date):\n",
                "    final_combined_df = None\n",
                "\n",
                "    try:\n",
                "        maintenance_date_folders = mssparkutils.fs.ls(base_path)\n",
                "        for folder in maintenance_date_folders:\n",
                "            folder_name = folder.name\n",
                "\n",
                "            # Check if the folder name starts with \"maintenancedate_\"\n",
                "            if folder_name.startswith(\"maintenancedate_\"):\n",
                "                # Extract the date part from the folder name\n",
                "                folder_date_str = folder_name.replace(\"maintenancedate_\", \"\").strip(\"/\")\n",
                "                folder_date = datetime.strptime(folder_date_str, \"%Y-%m-%d\")\n",
                "\n",
                "                # Process only folders with dates on or after the start date\n",
                "                if folder_date >= start_date:\n",
                "                    maintenance_date_path = folder.path\n",
                "\n",
                "                    # Append 'TableAnalysis' to the maintenance_date_path\n",
                "                    table_analysis_path = f\"{maintenance_date_path}/TableAnalysis\"\n",
                "\n",
                "                    # List all subfolders under the TableAnalysis folder\n",
                "                    try:\n",
                "                        subfolders = mssparkutils.fs.ls(table_analysis_path)\n",
                "                        for subfolder in subfolders:\n",
                "                            subfolder_path = subfolder.path\n",
                "\n",
                "                            # Path to the \"tablesSize\" folder for the given time period\n",
                "                            tables_fullanalysis_path = f\"{subfolder_path}/{time_period}/tablesSize\"\n",
                "\n",
                "                            # Check if the path exists\n",
                "                            if mssparkutils.fs.exists(tables_fullanalysis_path):\n",
                "                                try:\n",
                "                                    # Read the parquet files from the folder\n",
                "                                    df = spark.read.parquet(tables_fullanalysis_path)\n",
                "\n",
                "                                    # Add the maintenance date as a new column\n",
                "                                    df = df.withColumn(\"MaintenanceDate\", lit(folder_date_str).cast(\"date\"))\n",
                "\n",
                "                                    # Combine data from all folders\n",
                "                                    if final_combined_df is None:\n",
                "                                        final_combined_df = df\n",
                "                                    else:\n",
                "                                        final_combined_df = final_combined_df.union(df)\n",
                "\n",
                "                                    print(f\"Successfully read data from {tables_fullanalysis_path}\")\n",
                "                                except Exception as e:\n",
                "                                    print(f\"Failed to read data from {tables_fullanalysis_path}: {e}\")\n",
                "                            else:\n",
                "                                print(f\"Path does not exist: {tables_fullanalysis_path}. Skipping...\")\n",
                "                    except Exception as e:\n",
                "                        print(f\"Failed to list subfolders in {table_analysis_path}: {e}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to list maintenancedate_* subfolders in {base_path}: {e}\")\n",
                "\n",
                "    if final_combined_df is not None:\n",
                "        final_combined_df = final_combined_df.withColumn(\"MaintenanceTime\", lit(time_period.capitalize()))\n",
                "\n",
                "    return final_combined_df\n",
                "\n",
                "# Process data for \"before\" and \"after\"\n",
                "before_final_combined_df = process_data(base_path, \"before\", start_date)\n",
                "after_final_combined_df = process_data(base_path, \"after\", start_date)\n",
                "\n",
                "# Combine both DataFrames\n",
                "df_tableSize = None\n",
                "if before_final_combined_df and after_final_combined_df:\n",
                "    df_tableSize = before_final_combined_df.union(after_final_combined_df)\n",
                "elif before_final_combined_df:\n",
                "    df_tableSize = before_final_combined_df\n",
                "elif after_final_combined_df:\n",
                "    df_tableSize = after_final_combined_df\n",
                "\n",
                "# Write the combined DataFrame back to the Delta table\n",
                "delta_table_path = \"Tables/Maintenance_tablesSize\"\n",
                "if df_tableSize is not None:\n",
                "    df_tableSize.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
                "    print(f\"Data successfully loaded into Delta table at {delta_table_path}\")\n",
                "else:\n",
                "    print(\"No data was loaded. Check the paths and files in the Lakehouse.\")\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "7c64ebc1-e1ba-43f5-97ed-1633ae710ee1"
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Load Performance Baseline Data"
            ],
            "metadata": {},
            "id": "b1efa2e2-dc6b-4eb0-9884-c44259f07f46"
        },
        {
            "cell_type": "code",
            "source": [
                "from datetime import datetime\n",
                "from pyspark.sql.functions import lit, col\n",
                "from pyspark.sql import DataFrame\n",
                "\n",
                "# Define the base path of the Lakehouse\n",
                "base_path = \"Files/Maintenance/TablesMaintenance\"\n",
                "\n",
                "# Define the starting date\n",
                "start_date = datetime.strptime(\"2025-01-21\", \"%Y-%m-%d\")\n",
                "\n",
                "# Helper function to read and process parquet files\n",
                "def process_folder(base_path: str, subfolder_name: str, performance_type: str, start_date: datetime) -> DataFrame:\n",
                "    df_result = None\n",
                "    try:\n",
                "        maintenance_date_folders = mssparkutils.fs.ls(base_path)\n",
                "        for folder in maintenance_date_folders:\n",
                "            folder_name = folder.name\n",
                "            # Check if the folder name starts with \"maintenancedate_\"\n",
                "            if folder_name.startswith(\"maintenancedate_\"):\n",
                "                folder_date_str = folder_name.replace(\"maintenancedate_\", \"\").strip(\"/\")\n",
                "                folder_date = datetime.strptime(folder_date_str, \"%Y-%m-%d\")\n",
                "\n",
                "                # Process only folders with dates on or after the start date\n",
                "                if folder_date >= start_date:\n",
                "                    maintenance_date_path = folder.path\n",
                "                    perf_baseline_path = f\"{maintenance_date_path}/PerfBaseline\"\n",
                "\n",
                "                    try:\n",
                "                        guid_folders = mssparkutils.fs.ls(perf_baseline_path)\n",
                "                        for guid_folder in guid_folders:\n",
                "                            guid_path = guid_folder.path\n",
                "\n",
                "                            # Path to the performance metric folder (no 'before' folder anymore)\n",
                "                            target_path = f\"{guid_path}/{subfolder_name}\"\n",
                "\n",
                "                            # Check if the target path exists\n",
                "                            if mssparkutils.fs.exists(target_path):\n",
                "                                try:\n",
                "                                    # Read the parquet files\n",
                "                                    df_temp = spark.read.parquet(target_path)\n",
                "                                    df_temp = (\n",
                "                                        df_temp\n",
                "                                        .withColumn(\"MaintenanceDate\", lit(folder_date_str).cast(\"date\"))\n",
                "                                        .withColumn(\"PerformanceType\", lit(performance_type))\n",
                "                                    )\n",
                "\n",
                "                                    # Append or union the DataFrame\n",
                "                                    df_result = df_temp if df_result is None else df_result.union(df_temp)\n",
                "                                    print(f\"Successfully read data from {target_path}\")\n",
                "                                except Exception as e:\n",
                "                                    print(f\"Failed to read data from {target_path}: {e}\")\n",
                "                            else:\n",
                "                                print(f\"Path does not exist: {target_path}. Skipping...\")\n",
                "                    except Exception as e:\n",
                "                        print(f\"Failed to list GUID folders in {perf_baseline_path}: {e}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to list maintenancedate_* subfolders in {base_path}: {e}\")\n",
                "    return df_result\n",
                "\n",
                "# Process each performance type\n",
                "df_AllocatedCPU = process_folder(base_path, \"TopAllocatedCPU\", \"Allocated CPU\", start_date)\n",
                "df_EllapsedTime = process_folder(base_path, \"TopEllapsedTime\", \"EllapsedTime\", start_date)\n",
                "df_RemoteStorage = process_folder(base_path, \"TopRemoteStorage\", \"RemoteStorage\", start_date)\n",
                "df_RowCount = process_folder(base_path, \"TopRowCount\", \"RowCount\", start_date)\n",
                "df_ScannedMemory = process_folder(base_path, \"TopScannedMemory\", \"ScannedMemory\", start_date)\n",
                "\n",
                "# Combine all performance types into a single DataFrame\n",
                "df_Maintenance_Performance = (\n",
                "    df_AllocatedCPU\n",
                "    .union(df_EllapsedTime)\n",
                "    .union(df_RemoteStorage)\n",
                "    .union(df_RowCount)\n",
                "    .union(df_ScannedMemory)\n",
                ")\n",
                "df_Maintenance_Performance = df_Maintenance_Performance.drop(\"label\", \"rownum\")\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "64a5b456-572b-45c6-9622-33f0b394a5c5"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Translate Dataset name and Report name**"
            ],
            "metadata": {},
            "id": "fbe9b93f-9e76-4efe-a55c-8ca7246e51de"
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Specify the workspace(s) to the Dataset name and Report Name. For example:\n",
                "\n",
                "#####  List of workspaces \n",
                "workspaces = [\"workspace 1\", \"workspace 2\"]   # Add many as required\n"
            ],
            "metadata": {},
            "id": "32454747-898f-4bff-8829-a8e18dd76cda"
        },
        {
            "cell_type": "code",
            "source": [
                "#  List of workspaces \n",
                "workspaces = [\"OnyxTools-Test\"]"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "92bb9daa-2a32-46ad-83f9-47fbbc5faf1e"
        },
        {
            "cell_type": "code",
            "source": [
                "# Translate Dataset Names\n",
                "\n",
                "import sempy.fabric as fabric\n",
                "from pyspark.sql.functions import lit\n",
                "# Function to generate a DataFrame for a given workspace\n",
                "def create_dataset_dataframe(workspace_name):\n",
                "    dataset_data = fabric.list_datasets(workspace_name)\n",
                "    return (\n",
                "        spark.createDataFrame(dataset_data)\n",
                "        .withColumnRenamed(\"Dataset ID\", \"DatasetID\")\n",
                "        .withColumnRenamed(\"Dataset Name\", \"Dataset_Name\")\n",
                "        .withColumn(\"Dataset_Workspace\", lit(workspace_name))\n",
                "    )\n",
                "\n",
                "# Generate and union all report DataFrames\n",
                "dataset_Name_df = None\n",
                "for workspace in workspaces:\n",
                "    workspace_df = create_dataset_dataframe(workspace)\n",
                "    dataset_Name_df = workspace_df if dataset_Name_df is None else dataset_Name_df.union(workspace_df)\n",
                "    # display(dataset_Name_df)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "e275f0d1-5dcf-47ae-9691-6225e5dc52f3"
        },
        {
            "cell_type": "code",
            "source": [
                "# Translate Report Names\n",
                "\n",
                "import sempy.fabric as fabric\n",
                "from pyspark.sql.functions import lit\n",
                "\n",
                "# Function to generate a DataFrame for a given workspace\n",
                "def create_report_dataframe(workspace_name):\n",
                "    report_data = fabric.list_reports(workspace_name)\n",
                "    return (\n",
                "        spark.createDataFrame(report_data)\n",
                "        .withColumnRenamed(\"Id\", \"ReportId\")\n",
                "        .withColumnRenamed(\"Name\", \"Report_Name\")\n",
                "        .withColumn(\"Report_Workspace\", lit(workspace_name))\n",
                "    )\n",
                "\n",
                "# Generate and union all report DataFrames\n",
                "report_name_df = None\n",
                "for workspace in workspaces:\n",
                "    workspace_df = create_report_dataframe(workspace)\n",
                "    report_name_df = workspace_df if report_name_df is None else report_name_df.union(workspace_df)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "5c26de37-6873-4c14-a293-0b58566c86bd"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Joining Performance Data to Report Name and Dataset name"
            ],
            "metadata": {},
            "id": "5c119d87-7b7a-4954-9c7e-676ae8c0a461"
        },
        {
            "cell_type": "code",
            "source": [
                "# Join with dataset DataFrame \n",
                "dataset_df = (\n",
                "    df_Maintenance_Performance.join(\n",
                "        dataset_Name_df.select(\"DatasetId\", \"Dataset_Name\"), \n",
                "        on=\"DatasetId\", \n",
                "        how=\"left\"\n",
                "    )\n",
                "    .select(df_Maintenance_Performance[\"*\"], dataset_Name_df[\"Dataset_Name\"])\n",
                ")\n",
                "\n",
                "# Final join with report DataFrame\n",
                "final_df = (\n",
                "    dataset_df.join(\n",
                "        report_name_df.select(\"ReportId\", \"Report_Name\", \"Report_Workspace\"),\n",
                "        on=\"ReportId\",\n",
                "        how=\"left\"\n",
                "    )\n",
                "    .select(\n",
                "        dataset_df[\"*\"],\n",
                "        report_name_df[\"Report_Name\"],\n",
                "        report_name_df[\"Report_Workspace\"]\n",
                "    )\n",
                ")\n",
                "\n",
                "#display(final_df)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "649b3f73-6eab-46f0-a832-19c4aab3d674"
        },
        {
            "cell_type": "code",
            "source": [
                "# Write the combined DataFrame back to the Delta table\n",
                "delta_table_path = \"Tables/Maintenance_Performance\"\n",
                "\n",
                "if final_df is not None:\n",
                "    final_df.write \\\n",
                "        .format(\"delta\") \\\n",
                "        .mode(\"overwrite\") \\\n",
                "        .option(\"overwriteSchema\", \"true\") \\\n",
                "        .save(delta_table_path)\n",
                "    print(f\"Data successfully loaded into Delta table at {delta_table_path}\")\n",
                "else:\n",
                "    print(\"No data was loaded. Check the paths and files in the Lakehouse.\")\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "ac2a1d0f-1859-4fd3-ad50-e8b7ba676acf"
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "a365ComputeOptions": null,
        "sessionKeepAliveTimeout": 0,
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "dependencies": {
            "lakehouse": {
                "default_lakehouse": "83037cfc-6791-4ac4-ae26-38075549718d",
                "default_lakehouse_name": "OnyxToolsLake",
                "default_lakehouse_workspace_id": "0409393c-944d-4b64-9222-d4017c7466d0",
                "known_lakehouses": [
                    {
                        "id": "83037cfc-6791-4ac4-ae26-38075549718d"
                    }
                ]
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}