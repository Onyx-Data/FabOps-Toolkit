{
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "beforeAfter=\"before\"\n",
                "\n",
                "distinct_folder=\"\""
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ],
                "run_control": {
                    "frozen": false
                },
                "editable": true,
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "0101123b-685c-4128-8e9b-e0477b8bac4f"
        },
        {
            "cell_type": "code",
            "source": [
                "import uuid\n",
                "import re\n",
                "\n",
                "guid = str(uuid.uuid4())\n",
                "if distinct_folder==\"\":\n",
                "    distinct_folder = re.sub(r'[^a-zA-Z0-9]', '', guid)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "af7761b0-8c46-4166-af98-5816df1a1ee2"
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "from datetime import datetime\n",
                "\n",
                "if lakehouse_path:\n",
                "    try:\n",
                "        print(f\"üîç Analyzing folder structure for lakehouse: {lakehouse_name}\")\n",
                "        \n",
                "        local_path = lakehouse_path.replace(\"file://\", \"\")\n",
                "        \n",
                "        base_path = os.path.join(local_path, \"Tables\")\n",
                "        \n",
                "        def analyze_folder_structure(base_path):\n",
                "            result = []\n",
                "            for table in os.listdir(base_path):\n",
                "                table_path = os.path.join(base_path, table)\n",
                "                if os.path.isdir(table_path):\n",
                "                    total_partitions = 0\n",
                "                    total_files = 0\n",
                "                    \n",
                "                    for root, dirs, files in os.walk(table_path):\n",
                "                        relative_path = os.path.relpath(root, table_path)\n",
                "                        partition_level = relative_path.split(os.sep)\n",
                "                        partition_name = \"/\".join(partition_level)\n",
                "                        num_files = len(files)\n",
                "                        result.append({\n",
                "                            \"table\": table,\n",
                "                            \"partition\": partition_name,\n",
                "                            \"num_files\": num_files,\n",
                "                            \"partition_level\": len(partition_level)\n",
                "                        })\n",
                "                        if 'delta_log' not in partition_name:\n",
                "                            if partition_name!='.':\n",
                "                                total_partitions += 1\n",
                "                            total_files += num_files\n",
                "                    \n",
                "                    result.append({\n",
                "                        \"table\": table,\n",
                "                        \"partition\": \"TOTAL\",\n",
                "                        \"num_files\": total_files,\n",
                "                        \"partition_level\": \"N/A\"\n",
                "                    })\n",
                "            \n",
                "            return result\n",
                "\n",
                "        analysis_result = analyze_folder_structure(base_path)\n",
                "\n",
                "        if analysis_result:\n",
                "                df = spark.createDataFrame(analysis_result)\n",
                "                print(\"‚úÖ Analysis complete! Displaying results:\")\n",
                "                df.show(truncate=False)\n",
                "\n",
                "                print(f\"\\nüìà Summary for lakehouse '{lakehouse_name}':\")\n",
                "                tables_analyzed = len(set([item['table'] for item in analysis_result if item['partition'] != 'TOTAL']))\n",
                "                total_partitions = sum([item['num_files'] for item in analysis_result if item['partition'] == 'TOTAL'])\n",
                "                print(f\"  - Source lakehouse: {lakehouse_name}\")\n",
                "                print(f\"  - Tables analyzed: {tables_analyzed}\")\n",
                "                print(f\"  - Total files across all tables: {total_partitions}\")\n",
                "                \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error analyzing folder structure: {str(e)}\")\n",
                "else:\n",
                "    print(\"‚ùå Cannot analyze folder structure - lakehouse not mounted\")"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "a9e65364-1be6-416c-ae9f-f940647d7829"
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.functions import col, lit, current_timestamp\n",
                "from datetime import datetime\n",
                "import uuid\n",
                "\n",
                "if 'df' in locals() and df is not None and lakehouse_path:\n",
                "        print(\"üíæ Preparing to save analysis results to the default lakehouse...\")\n",
                "        maintenance_folder = \"Files/Maintenance/TablesMaintenance/\"\n",
                "        current_date = datetime.now().strftime('%Y-%m-%d')\n",
                "        maintenance_date = datetime.now().strftime('%Y%m%d')\n",
                "        maintenance_date_folder = f\"maintenancedate_{current_date}\"\n",
                "\n",
                "        df2 = df.filter(\n",
                "            (col(\"partition\") == 'TOTAL') |\n",
                "            (col(\"partition\") == '.') |\n",
                "            (col(\"partition\").contains('delta'))\n",
                "        )\n",
                "\n",
                "        df.write.format('parquet').save(\n",
                "            f\"{maintenance_folder}{maintenance_date_folder}/TableAnalysis/{distinct_folder}/{beforeAfter}/tablesFullanalysis/\"\n",
                "        )\n",
                "        df2.write.format('parquet').save(\n",
                "            f\"{maintenance_folder}{maintenance_date_folder}/TableAnalysis/{distinct_folder}/{beforeAfter}/tablesSize/\"\n",
                "        )\n",
                "        mssparkutils.notebook.exit(distinct_folder)\n",
                "\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "a09fca46-1698-4af4-b680-75dee13b2618"
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "a365ComputeOptions": null,
        "sessionKeepAliveTimeout": 0,
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "dependencies": {
            "lakehouse": null
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}